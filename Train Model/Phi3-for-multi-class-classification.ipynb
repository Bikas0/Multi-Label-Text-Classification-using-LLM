{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8675326,"sourceType":"datasetVersion","datasetId":5199933}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install the missing module\n!pip install -q accelerate -U bitsandbytes\n!pip install -q scikit-multilearn datasets peft transformers\nimport os\nimport random\nimport functools\nimport csv\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score\nfrom skmultilearn.model_selection import iterative_train_test_split # This import should now work\nfrom datasets import Dataset, DatasetDict\nfrom peft import (\n    LoraConfig,\n    prepare_model_for_kbit_training,\n    get_peft_model\n)\nimport os\nos.environ[\"WANDB_MODE\"] = \"disabled\"\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-12T16:47:55.535472Z","iopub.execute_input":"2024-06-12T16:47:55.535869Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning:\n\nos.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_examples(examples, tokenizer):\n    tokenized_inputs = tokenizer(examples['text'])\n    tokenized_inputs['labels'] = examples['labels']\n    return tokenized_inputs\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define custom batch preprocessor\ndef collate_fn(batch, tokenizer):\n    dict_keys = ['input_ids', 'attention_mask', 'labels']\n    d = {k: [dic[k] for dic in batch] for k in dict_keys}\n    d['input_ids'] = torch.nn.utils.rnn.pad_sequence(\n        d['input_ids'], batch_first=True, padding_value=tokenizer.pad_token_id\n    )\n    d['attention_mask'] = torch.nn.utils.rnn.pad_sequence(\n        d['attention_mask'], batch_first=True, padding_value=0\n    )\n    d['labels'] = torch.stack(d['labels'])\n    return d\n\n\n# define which metrics to compute for evaluation\n# def compute_metrics(p):\n#     predictions, labels = p\n#     f1_micro = f1_score(labels, predictions > 0, average = 'micro')\n#     f1_macro = f1_score(labels, predictions > 0, average = 'macro')\n#     f1_weighted = f1_score(labels, predictions > 0, average = 'weighted')\n#     return {\n#         'f1_micro': f1_micro,\n#         'f1_macro': f1_macro,\n#         'f1_weighted': f1_weighted\n#     }\n\n\n\n# import torch\n# from sklearn.metrics import f1_score, precision_score, recall_score\n\n# # Define custom batch preprocessor\n# def collate_fn(batch, tokenizer):\n#     dict_keys = ['input_ids', 'attention_mask', 'labels']\n#     d = {k: [dic[k] for dic in batch] for k in dict_keys}\n#     d['input_ids'] = torch.nn.utils.rnn.pad_sequence(\n#         d['input_ids'], batch_first=True, padding_value=tokenizer.pad_token_id\n#     )\n#     d['attention_mask'] = torch.nn.utils.rnn.pad_sequence(\n#         d['attention_mask'], batch_first=True, padding_value=0\n#     )\n#     d['labels'] = torch.stack(d['labels'])\n#     return d\n\n# # Define which metrics to compute for evaluation\n# def compute_metrics(p):\n#     predictions, labels = p\n#     thresholded_predictions = predictions > 0\n\n#     f1_micro = f1_score(labels, thresholded_predictions, average='micro')\n#     f1_macro = f1_score(labels, thresholded_predictions, average='macro')\n#     f1_weighted = f1_score(labels, thresholded_predictions, average='weighted')\n\n#     precision_micro = precision_score(labels, thresholded_predictions, average='micro')\n#     precision_macro = precision_score(labels, thresholded_predictions, average='macro')\n#     precision_weighted = precision_score(labels, thresholded_predictions, average='weighted')\n\n#     recall_micro = recall_score(labels, thresholded_predictions, average='micro')\n#     recall_macro = recall_score(labels, thresholded_predictions, average='macro')\n#     recall_weighted = recall_score(labels, thresholded_predictions, average='weighted')\n\n#     return {\n#         'f1_micro': f1_micro,\n#         'f1_macro': f1_macro,\n#         'f1_weighted': f1_weighted,\n#         'precision_micro': precision_micro,\n#         'precision_macro': precision_macro,\n#         'precision_weighted': precision_weighted,\n#         'recall_micro': recall_micro,\n#         'recall_macro': recall_macro,\n#         'recall_weighted': recall_weighted\n#     }\n\n# =======================================+===============================\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n\n# Define which metrics to compute for evaluation\ndef compute_metrics(p, class_names):\n    predictions, labels = p\n    thresholded_predictions = predictions > 0\n\n#     accuracy = accuracy_score(labels, thresholded_predictions)\n\n    f1_micro = f1_score(labels, thresholded_predictions, average='micro')\n\n    precision_micro = precision_score(labels, thresholded_predictions, average='micro')\n\n    recall_micro = recall_score(labels, thresholded_predictions, average='micro')\n\n    # Compute per-class metrics\n    f1_per_class = f1_score(labels, thresholded_predictions, average=None)\n    precision_per_class = precision_score(labels, thresholded_predictions, average=None)\n    recall_per_class = recall_score(labels, thresholded_predictions, average=None)\n\n    # Map metrics to class names\n    f1_per_class_dict = {class_names[i]: f1_per_class[i] for i in range(len(class_names))}\n    precision_per_class_dict = {class_names[i]: precision_per_class[i] for i in range(len(class_names))}\n    recall_per_class_dict = {class_names[i]: recall_per_class[i] for i in range(len(class_names))}\n\n    return {\n        'f1_micro': f1_micro,\n        'precision_micro': precision_micro,\n        'recall_micro': recall_micro,\n        'f1_per_class': f1_per_class_dict,\n        'precision_per_class': precision_per_class_dict,\n        'recall_per_class': recall_per_class_dict\n    }\n\n\n# ++++++++++++++++++++++++++++++++++++++++\n# from sklearn.metrics import f1_score, precision_score, recall_score\n\n# # Define custom batch preprocessor\n# def collate_fn(batch, tokenizer):\n#     dict_keys = ['input_ids', 'attention_mask', 'labels']\n#     d = {k: [dic[k] for dic in batch] for k in dict_keys}\n#     d['input_ids'] = torch.nn.utils.rnn.pad_sequence(\n#         d['input_ids'], batch_first=True, padding_value=tokenizer.pad_token_id\n#     )\n#     d['attention_mask'] = torch.nn.utils.rnn.pad_sequence(\n#         d['attention_mask'], batch_first=True, padding_value=0\n#     )\n#     d['labels'] = torch.stack(d['labels'])\n#     return d\n\n# # Define which metrics to compute for evaluation\n# def compute_metrics(p, class_names):\n#     predictions, labels = p\n#     thresholded_predictions = predictions > 0\n\n#     f1_micro = f1_score(labels, thresholded_predictions, average='micro')\n#     f1_weighted = f1_score(labels, thresholded_predictions, average='weighted')\n\n#     precision_micro = precision_score(labels, thresholded_predictions, average='micro')\n#     precision_weighted = precision_score(labels, thresholded_predictions, average='weighted')\n\n#     recall_micro = recall_score(labels, thresholded_predictions, average='micro')\n#     recall_weighted = recall_score(labels, thresholded_predictions, average='weighted')\n\n#     # Compute per-class metrics\n#     f1_per_class = f1_score(labels, thresholded_predictions, average=None)\n#     precision_per_class = precision_score(labels, thresholded_predictions, average=None)\n#     recall_per_class = recall_score(labels, thresholded_predictions, average=None)\n\n#     # Create a dictionary with class names and their respective metrics\n#     per_class_metrics = {}\n#     for idx, class_name in enumerate(class_names):\n#         per_class_metrics[class_name] = {\n#             'f1': f1_per_class[idx],\n#             'precision': precision_per_class[idx],\n#             'recall': recall_per_class[idx]\n#         }\n\n#     return {\n#         'f1_micro': f1_micro,\n#         'f1_weighted': f1_weighted,\n#         'precision_micro': precision_micro,\n#         'precision_weighted': precision_weighted,\n#         'recall_micro': recall_micro,\n#         'recall_weighted': recall_weighted,\n#         'per_class_metrics': per_class_metrics\n#     }\n\n\n# ======================================================\n# import torch\n# from sklearn.metrics import f1_score, precision_score, recall_score\n\n# # Define custom batch preprocessor\n# def collate_fn(batch, tokenizer):\n#     dict_keys = ['input_ids', 'attention_mask', 'labels']\n#     d = {k: [dic[k] for dic in batch] for k in dict_keys}\n#     d['input_ids'] = torch.nn.utils.rnn.pad_sequence(\n#         d['input_ids'], batch_first=True, padding_value=tokenizer.pad_token_id\n#     )\n#     d['attention_mask'] = torch.nn.utils.rnn.pad_sequence(\n#         d['attention_mask'], batch_first=True, padding_value=0\n#     )\n#     d['labels'] = torch.stack(d['labels'])\n#     return d\n\n# # Define which metrics to compute for evaluation\n# def compute_metrics(p):\n#     predictions, labels = p\n#     thresholded_predictions = predictions > 0\n\n#     f1_micro = f1_score(labels, thresholded_predictions, average='micro')\n#     # f1_macro = f1_score(labels, thresholded_predictions, average='macro')\n#     f1_weighted = f1_score(labels, thresholded_predictions, average='weighted')\n\n#     precision_micro = precision_score(labels, thresholded_predictions, average='micro')\n#     # precision_macro = precision_score(labels, thresholded_predictions, average='macro')\n#     precision_weighted = precision_score(labels, thresholded_predictions, average='weighted')\n\n#     recall_micro = recall_score(labels, thresholded_predictions, average='micro')\n#     # recall_macro = recall_score(labels, thresholded_predictions, average='macro')\n#     recall_weighted = recall_score(labels, thresholded_predictions, average='weighted')\n\n#     # Compute per-class metrics\n#     f1_per_class = f1_score(labels, thresholded_predictions, average=None)\n#     precision_per_class = precision_score(labels, thresholded_predictions, average=None)\n#     recall_per_class = recall_score(labels, thresholded_predictions, average=None)\n\n#     return {\n#         'f1_micro': f1_micro,\n#         # 'f1_macro': f1_macro,\n#         'f1_weighted': f1_weighted,\n#         'precision_micro': precision_micro,\n#         # 'precision_macro': precision_macro,\n#         'precision_weighted': precision_weighted,\n#         'recall_micro': recall_micro,\n#         # 'recall_macro': recall_macro,\n#         'recall_weighted': recall_weighted,\n#         'f1_per_class': f1_per_class,\n#         'precision_per_class': precision_per_class,\n#         'recall_per_class': recall_per_class\n#     }\n\n\n\n\n# create custom trainer class to be able to pass label weights and calculate mutilabel loss\nclass CustomTrainer(Trainer):\n\n    def __init__(self, label_weights, **kwargs):\n        super().__init__(**kwargs)\n        self.label_weights = label_weights\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n\n        # forward pass\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n\n        # compute custom loss\n        loss = F.binary_cross_entropy_with_logits(logits, labels.to(torch.float32), pos_weight=self.label_weights)\n        return (loss, outputs) if return_outputs else loss\n\n# set random seed\nrandom.seed(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport re\ndata = pd.read_csv(\"/kaggle/input/dataset/reviewsDataset.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    # Regular expression to match only Bengali characters, digits, spaces, and %\n    pattern = re.compile(r'[^০-৯\\u0980-\\u09FF\\s%]')\n    filtered_string = pattern.sub('', text)\n    output_string = re.sub(r'\\s+', ' ', filtered_string).strip()\n    return output_string\n\n# Apply the function to the 'text' column\ndata['reviewContent'] = data['reviewContent'].apply(clean_text)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a set to hold unique aspects\nunique_aspects = set()\n\n# Split the aspects and update the unique_aspects set\ndata['ground_truth_aspects'].str.split(', ').apply(unique_aspects.update)\n\n# Strip spaces from unique aspects and remove any empty strings\nunique_aspects = {aspect.strip() for aspect in unique_aspects if aspect.strip()}\n\n# Create a new column for each unique aspect and initialize with 0\nfor aspect in unique_aspects:\n    data[aspect] = 0\n\n# Populate the columns based on the aspects present in each row\nfor index, row in data.iterrows():\n    aspects = [aspect.strip() for aspect in row['ground_truth_aspects'].split(', ')]\n    for aspect in aspects:\n        if aspect:  # only update if aspect is not an empty string\n            data.at[index, aspect] = 1\n\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = ['seller', 'delivery', 'service', 'price', 'packaging',\n       'shelf', 'rider', 'product']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\ndef plot_value_counts(df, column_name):\n    # Get value counts\n    value_counts = df[column_name].value_counts().reset_index()\n    value_counts.columns = [column_name, 'count']\n\n    # Create bar plot with different colors\n    fig_bar = px.bar(value_counts, x=column_name, y='count', \n                     color=column_name, \n                     labels={column_name:'Value', 'count':'Count'},\n                     title=f'{column_name.capitalize()} Value Counts')\n\n    # Update the layout for better visualization\n    fig_bar.update_layout(showlegend=False)\n\n    fig_bar.show()\n\n    # Create pie chart\n    fig_pie = px.pie(value_counts, names=column_name, values='count', \n                     title=f'{column_name.capitalize()} Value Counts Distribution')\n\n    fig_pie.show()\n\n# Example usage:\nfor col in columns:\n    plot_value_counts(data, col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_columns = [\"ground_truth_aspects\", \"packaging  product\", \"seller,shelf\", \"product,price\"]\ndata = data.drop(columns=drop_columns, axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text, labels = list(zip(*[(f'ReviewContent: {row[0].strip()}', row[1:].astype(int)) for row in data.values])) # Iterate over the values of the DataFrame and convert the labels to integers\nlabels = np.array(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create label weights\nlabel_weights = 1 - labels.sum(axis=0) / labels.sum()\n\n# stratified train test split for multilabel ds\nrow_ids = np.arange(len(labels))\ntrain_idx, y_train, val_idx, y_val = iterative_train_test_split(row_ids[:,np.newaxis], labels, test_size = 0.1)\nx_train = [text[i] for i in train_idx.flatten()]\nx_val = [text[i] for i in val_idx.flatten()]\n\n# create hf dataset\nds = DatasetDict({\n    'train': Dataset.from_dict({'text': x_train, 'labels': y_train}),\n    'val': Dataset.from_dict({'text': x_val, 'labels': y_val})\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(\"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\")\n# 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","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model name\n# model_name = 'meta-llama/Meta-Llama-3-8B'\nmodel_name = 'microsoft/Phi-3-mini-4k-instruct'\n# preprocess dataset with tokenizer\ndef tokenize_examples(examples, tokenizer):\n    tokenized_inputs = tokenizer(examples['text'])\n    tokenized_inputs['labels'] = examples['labels']\n    return tokenized_inputs\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\ntokenized_ds = ds.map(functools.partial(tokenize_examples, tokenizer=tokenizer), batched=True)\ntokenized_ds = tokenized_ds.with_format('torch')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install -q bitsandbytes peft\nfrom transformers import BitsAndBytesConfig\nfrom peft import get_peft_model, LoraConfig\n# qunatization config\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit = True, # enable 4-bit quantization\n    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n)\n\n# lora config\nlora_config = LoraConfig(\n    r = 16, # the dimension of the low-rank matrices\n    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n    lora_dropout = 0.05, # dropout probability of the LoRA layers\n    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n    task_type = 'SEQ_CLS'\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom transformers import BitsAndBytesConfig\nfrom peft import get_peft_model, LoraConfig\n# qunatization config\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit = True, # enable 4-bit quantization\n    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n)\n\n# lora config\nlora_config = LoraConfig(\n    r = 16, # the dimension of the low-rank matrices\n    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n    lora_dropout = 0.05, # dropout probability of the LoRA layers\n    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n    task_type = 'SEQ_CLS'\n)\n\n# load model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    quantization_config=quantization_config,\n    num_labels=labels.shape[1]\n)\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\nmodel.config.pad_token_id = tokenizer.pad_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = ['seller', 'delivery', 'service', 'price', 'packaging',\n       'shelf', 'rider', 'product']\n\ntraining_args = TrainingArguments(\n    output_dir='multilabel_classification',\n    learning_rate=1e-4,\n    per_device_train_batch_size=4,  # tested with 16GB GPU RAM\n    per_device_eval_batch_size=4,\n    num_train_epochs=20,\n    weight_decay=0.01,\n    evaluation_strategy='steps',\n    save_strategy='steps',\n    load_best_model_at_end=True,\n    logging_steps=500,  # log every 250 steps (500 is a round multiple of 250)\n    save_steps=2000,  # save every 500 steps\n    logging_dir='logs',  # directory for storing logs\n)\n\n# Instantiate the trainer with the custom implementation\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds['train'],\n    eval_dataset=tokenized_ds['val'],\n    tokenizer=tokenizer,\n    data_collator=functools.partial(collate_fn, tokenizer=tokenizer),\n    compute_metrics=lambda p: compute_metrics(p, class_names),\n    label_weights=torch.tensor(label_weights, device=model.device) # Pass label_weights here\n)\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}